{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sC-LZ20S_WUr"
   },
   "source": [
    "# Assignment 2: Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9xqCFJBv_WUt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\Ger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iq8sr9x17KhU"
   },
   "source": [
    "## Task 1: Named Entity Annotation (10 Marks)\n",
    "\n",
    "Using the IOB tagging scheme annotate all of the named entities (PERson, LOCation, ORGanisation, TIME) in the following sentence:\n",
    "\n",
    "*Wayne Rooney is a professional footballer from England who last played for Major League Soccer club D.C. United and will join Derby County in January 2020.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htlSW1ad81D-"
   },
   "source": [
    "Edit this cell and write your annotation below the line. (Note that you don't have to write code for this task, you have to annotate it manually)\n",
    "\n",
    "---\n",
    "('Wayne Rooney', 'B-PERS'), ('is','O-VB'), ('a','O-DT'), ('professional', 'O-JJ'), ('footballer', O-NN), ('from', 'O-IN'), \n",
    "('England', 'B-LOC'), ('who', 'O-WP'), ('last', 'O-JJ'), ('played' 'O-VBD'), ('for', 'O-IN'), \n",
    "('Major League Soccer', 'B-ORG'), ('club', 'O-NN'), \n",
    "('D.C. United', 'B-ORG'), ('and', 'O-CC'), ('will', 'O-MD'), ('join', 'O-VB'),\n",
    "('Derby County', 'B-LOC'), ('in', 'O-IN'), ('January 2020', 'O-TIME'), ('.', 'O-.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZNmDWxj-V-J"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### For subsequent tasks in this assignment, you will work with the documents in `football_players.txt` to perform various information extraction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "V9YE4n6u7olU"
   },
   "outputs": [],
   "source": [
    "# Download the text file (uncomment the line below in this cell, if not already downloaded from Blackboard)\n",
    "# !curl \"https://ideone.com/plain/OvwDXZ\" > football_players.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpSaij2b73Vj"
   },
   "source": [
    " Read all the documents from `football_players.txt` into a list called `docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qteh89cs7q4x"
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "# your code goes here\n",
    "import csv\n",
    "with open('football_players.txt', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter = '.')\n",
    "    for i in reader:\n",
    "        docs.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCEJrJ-p_WU1"
   },
   "source": [
    "## Task 2 (10 Marks)\n",
    "Write a function that takes a document and returns a list of sentences with part-of-speech tags.\n",
    "\n",
    "Please keep in mind that the expected output is a list within a list as shown below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO7Fyfq7DYxW"
   },
   "source": [
    "Hint: For this task you need to perform three steps:\n",
    "1. Sentence Segmentation\n",
    "1. Word Tokenization\n",
    "1. Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U3MCJIcR_WU2"
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "def ie_preprocess(document):\n",
    "    # your code goes here\n",
    "    tagg_sent = []\n",
    "    for i in document:\n",
    "        lst = []\n",
    "        token = word_tokenize(i)\n",
    "        word_tag = pos_tag(token)\n",
    "        for x, y in word_tag:\n",
    "            lst.append(((x), (y)))\n",
    "        tagg_sent.append(lst)\n",
    "    return tagg_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-E04CUNb_WU6"
   },
   "source": [
    "Run the cell below to verify your result for the second sentence in the first document.\n",
    "Expected output: \n",
    "`[('He', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('forward', 'NN'), ('and', 'CC'), ('serves', 'NNS'), ('as', 'IN'), ('captain', 'NN'), ('for', 'IN'), ('Portugal', 'NNP'), ('.', '.')]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "R30taRgf_WU6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('forward', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('serves', 'NNS'),\n",
       " ('as', 'IN'),\n",
       " ('captain', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('Portugal', 'NNP')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_doc = docs[0]\n",
    "tagged_sentences = ie_preprocess(first_doc)\n",
    "tagged_sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYTwrZId_WU_"
   },
   "source": [
    "## Task 3 (20 Marks)\n",
    "Write a function that takes a list of tokens with POS tags for a sentence and returns a list of named entities (NE). \n",
    "\n",
    "Hint: Use `binary = True` while calling NE chunk function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5fC0iqJJ_WU_"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def find_named_entities(sent):\n",
    "    named_entities = []\n",
    "    # your code goes here\n",
    "    chunks = nltk.ne_chunk(sent, binary=True)\n",
    "    items = nltk.chunk.tree2conlltags(chunks)\n",
    "    for i in items:\n",
    "        named_entities.append((i[0],i[2]))\n",
    "    # code adapted from:\n",
    "    # https://stackoverflow.com/questions/30664677/extract-list-of-persons-and-organizations-using-stanford-ner-tagger-in-nltk\n",
    "    cont_chunk = []\n",
    "    curr_chunk = []\n",
    "    for token, tag in named_entities:\n",
    "        if tag != 'O':\n",
    "            curr_chunk.append((token, tag))\n",
    "        else:\n",
    "            if curr_chunk:\n",
    "                cont_chunk.append(curr_chunk)\n",
    "                curr_chunk = []\n",
    "        if curr_chunk:\n",
    "            cont_chunk.append(curr_chunk)\n",
    "    n_e = [\" \".join([token for token, tag in ne]) for ne in cont_chunk]\n",
    "    nam_ent = set()\n",
    "    named_entities = [i for i in n_e if not (i in nam_ent or nam_ent.add(i))] \n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Td5yJ8cgFScx"
   },
   "source": [
    "Run the cell below to verify your result for the first sentence in the first document.\n",
    "Expected output: `['Cristiano Ronaldo', 'Santos Aveiro', 'ComM', 'GOIH', 'Portuguese', 'Portuguese', 'Spanish', 'Real Madrid', 'Portugal']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FijjdAPWFsp2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cristiano Ronaldo',\n",
       " 'Santos Aveiro',\n",
       " 'ComM',\n",
       " 'GOIH',\n",
       " 'Portuguese',\n",
       " 'Spanish',\n",
       " 'Real Madrid',\n",
       " 'Portugal']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences = ie_preprocess(docs[0])\n",
    "find_named_entities(tagged_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHMp7xtK_WVE"
   },
   "source": [
    "## Task 4 (5 Marks)\n",
    "\n",
    "Implement the `find_all_named_entities` function below to find **all** NEs in a given document.\n",
    "\n",
    "Hint: Use `find_named_entities` implemented above for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TwFlzQx4_WVF"
   },
   "outputs": [],
   "source": [
    "def find_all_named_entities(doc):\n",
    "    named_entities = []\n",
    "    # your code goes here\n",
    "    if any(isinstance(i, list) for i in doc) == True:\n",
    "        for i in doc:\n",
    "            tagged_sent = ie_preprocess(i)\n",
    "            for i in tagged_sent:\n",
    "                ne = find_named_entities(i)\n",
    "                named_entities.append(ne)\n",
    "    \n",
    "    else:\n",
    "        tagged_sent = ie_preprocess(doc)\n",
    "        for i in tagged_sent:\n",
    "            ne = find_named_entities(i)\n",
    "            named_entities.append(ne)\n",
    "    \n",
    "    named_entities = [i for j in named_entities for i in j]\n",
    "    return named_entities   # return a flat list and not a list of lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdM0-LZlJy4u"
   },
   "source": [
    "How many named entities did you find in the first document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_ajmnnOqJ8V6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "num_ne = find_all_named_entities(docs[0])\n",
    "print(len(num_ne))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2AzD9MVNIx2"
   },
   "source": [
    "## Task 5 (5 Marks)\n",
    "\n",
    "Find named entities across **all** documents in `football_players.txt`, and save the result into a single flat list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YULMcK1-NSR9"
   },
   "outputs": [],
   "source": [
    "all_named_entities = []\n",
    "# your code goes here\n",
    "for i in docs:\n",
    "    tagged_sent = ie_preprocess(i)\n",
    "    for i in tagged_sent:\n",
    "        ne = find_named_entities(i)\n",
    "        all_named_entities.append(ne)\n",
    "\n",
    "all_named_entities = [i for j in all_named_entities for i in j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaM9Cs9zNGM2"
   },
   "source": [
    "How many named entities did you find across all documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jCNIrC_SNpHQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356\n"
     ]
    }
   ],
   "source": [
    "print(len(all_named_entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7-ma9lJ_WVJ"
   },
   "source": [
    "## Task 6 (40 Marks)\n",
    "\n",
    "Write functions to extract the name of the player, country of origin and date of birth as well as the following relations: team(s) of the player and position(s) of the player.\n",
    "\n",
    "Hint: Use the `re.compile()` function to create the extraction patterns.\n",
    "\n",
    "Reference: https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tbaFyiah_WVK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "java_path = r\"C:\\Program Files\\Java\\jdk-15.0.1\\bin\\java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "st = StanfordNERTagger(r'C:\\Users\\Ger\\Desktop\\College\\CT5146\\stanford-ner-4.2.0\\stanford-ner-2020-11-17\\classifiers\\english.muc.7class.distsim.crf.ser.gz',\n",
    "                       r'C:\\Users\\Ger\\Desktop\\College\\CT5146\\stanford-ner-4.2.0\\stanford-ner-2020-11-17\\stanford-ner.jar',\n",
    "                        encoding = 'utf-8')\n",
    "\n",
    "def name_of_the_player(doc):\n",
    "    name = []\n",
    "    n = []\n",
    "    if any(isinstance(i, list) for i in doc) == True:\n",
    "        for i in doc:\n",
    "            for j in i:\n",
    "                tokenized_text = word_tokenize(j)\n",
    "                classified_text = st.tag(tokenized_text)\n",
    "                people = [i for i in classified_text if i[1]=='PERSON']\n",
    "                if len(people) > 0: #take full names, i.e. two name elements as a minimum for full names\n",
    "                    n.append(' '.join([i[0] for i in people])) ##\n",
    "    \n",
    "        freq = Counter(n)\n",
    "        most_freq = freq.most_common(1)[0]\n",
    "       \n",
    "        for i in n:\n",
    "            j = str(i).split() # full name split\n",
    "            if len(j) > 1:\n",
    "                if j[0] == most_freq[0]: # to ensure we are not just referencing the first or last name\n",
    "                    pass\n",
    "                else:\n",
    "                    for a, b in enumerate(j):\n",
    "                        if most_freq[0] in b and len(b) > 1:\n",
    "                            name.append((' '.join((j[a-1],j[a])))) \n",
    "                \n",
    "            return list(set(name)) ##\n",
    "    \n",
    "    else:\n",
    "        for i in doc:\n",
    "            j = str(i)\n",
    "            tokenized_text = word_tokenize(j)\n",
    "            classified_text = st.tag(tokenized_text)\n",
    "            people = [i for i in classified_text if i[1]=='PERSON']\n",
    "            if len(people) > 0: #take full names, i.e. two name elements as a minimum for full names\n",
    "                n.append(' '.join([i[0] for i in people])) ##\n",
    "    \n",
    "        freq = Counter(n)\n",
    "        most_freq = freq.most_common(1)[0]\n",
    "    \n",
    "        for i in n:\n",
    "            j = i.split()\n",
    "            if most_freq[0] in j and len(j) > 1:\n",
    "                name.append(i)\n",
    "                \n",
    "        return list(name) #\n",
    "    \n",
    "def country_of_origin(doc):\n",
    "    player = name_of_the_player(doc)\n",
    "    if any(isinstance(i, list) for i in doc) == True:\n",
    "        tagged_words = []\n",
    "        for i in doc:\n",
    "            for j in i:\n",
    "                tokenized_text = word_tokenize(j)\n",
    "                classified_text = st.tag(tokenized_text)\n",
    "                if len(classified_text) > 0:\n",
    "                    tagged_words.append(classified_text)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "        root1 = []\n",
    "        for i in tagged_words:\n",
    "            for j in i:\n",
    "                # not great only identifies languages/nationality of countries with these endings\n",
    "                if j[0][-3:] in ('ish','ese','ine','ian'):\n",
    "                    word_root = j[0][:4]\n",
    "                    word = j[0][0]\n",
    "                    root1.append((word_root, word))\n",
    "\n",
    "        root2 = []\n",
    "        for i in tagged_words:\n",
    "            for j in i:\n",
    "                place = [x for x in j if j[1]=='LOCATION']\n",
    "                if len(place) > 0 and len(place[0]) > 4:\n",
    "                    word_root = place[0][:4]\n",
    "                    word = place[0]\n",
    "                    root2.append((word_root, word))\n",
    "    \n",
    "        cntry = []\n",
    "        for i in root1:\n",
    "            for j in root2:\n",
    "                if i[0] == j[0]:\n",
    "                    cntry.append(j[1])\n",
    "        \n",
    "        freq = Counter(cntry)\n",
    "        country = freq.most_common(1)[0]\n",
    "        \n",
    "        return ('Player: '+ str(player) + ' Country: '+ str(country))\n",
    "                                   \n",
    "    else:\n",
    "        tagged_words = []\n",
    "        for i in doc:\n",
    "            j = str(i)\n",
    "            tokenized_text = word_tokenize(j)\n",
    "            classified_text = st.tag(tokenized_text)\n",
    "            if len(classified_text) > 0:\n",
    "                tagged_words.append(classified_text)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        root1 = []\n",
    "        for i in tagged_words:\n",
    "            for j in i:\n",
    "            # not great only identifies languages/nationality of countries with these endings\n",
    "            # captures English/British, Portuguese, Argentine/Argentinian (not Spanish)\n",
    "            # could potentially add an edit distance to get Spain -> Spanish\n",
    "                if j[0][-3:] in ('ish','ese','ine','ian'):\n",
    "                    word_root = j[0][:4]\n",
    "                    word = j[0]\n",
    "                    root1.append((word_root, word))\n",
    "\n",
    "        root2 = []\n",
    "        for i in tagged_words:\n",
    "            for j in i:\n",
    "                place = [x for x in j if j[1]=='LOCATION']\n",
    "                if len(place) > 0 and len(place[0]) > 4:\n",
    "                    word_root = place[0][:4]\n",
    "                    word = place[0]\n",
    "                    root2.append((word_root, word))\n",
    "    \n",
    "        cntry = []\n",
    "        for i in root1:\n",
    "            for j in root2:\n",
    "                if i[0] == j[0]:\n",
    "                    cntry.append(j[1])\n",
    "        \n",
    "        freq = Counter(cntry)\n",
    "        country = freq.most_common(1)[0]\n",
    "        \n",
    "        return ('Player: '+ str(player[0]) + ' Country: '+ str(country[0]))\n",
    "\n",
    "def date_of_birth(doc):\n",
    "    player = name_of_the_player(doc)\n",
    "    if any(isinstance(i, list) for i in doc) == True:\n",
    "        for i in doc:\n",
    "            for j in i:\n",
    "                match_re = re.compile('[0-9]+\\s[A-Z][a-z]+\\s[0-9]+')\n",
    "                date_str = re.findall(match_re, j)\n",
    "                if len(date_str) > 0:\n",
    "            \n",
    "                    return ('Player: '+ str(player) + ' DOB: '+ str(date_str))\n",
    "\n",
    "    else:\n",
    "        for i in doc:\n",
    "            j = str(i)\n",
    "            match_re = re.compile('[0-9]+\\s[A-Z][a-z]+\\s[0-9]+')\n",
    "            date_str = re.findall(match_re, j)\n",
    "            if len(date_str) > 0:\n",
    "            \n",
    "                return ('Player: '+ str(player) + ' DOB: '+ str(date_str))\n",
    "\n",
    "def team_of_player(doc):\n",
    "    lst = []\n",
    "    player = name_of_the_player(doc)\n",
    "    for i in doc:\n",
    "        j = str(i).split()\n",
    "        for i in j:\n",
    "            k = i.strip('\"()[];,')\n",
    "            reg_exp = re.compile('[A-Za-z]+')\n",
    "            match_re = re.findall(reg_exp,k)\n",
    "            if len(match_re) == 1:\n",
    "                lst.append(match_re)\n",
    "\n",
    "    lst1 = [i for j in lst for i in j]\n",
    "    lst2 = (' '.join(lst1)).split('club',1)[1]\n",
    "    token_words = word_tokenize(lst2)\n",
    "    sent = ie_preprocess(token_words)\n",
    "\n",
    "    named_entities = []\n",
    "    for i in sent:\n",
    "        chunks = nltk.ne_chunk(i, binary=True)\n",
    "        items = nltk.chunk.tree2conlltags(chunks)\n",
    "        for i in items:\n",
    "            named_entities.append((i[0],i[2]))\n",
    "\n",
    "    cont_chunk = []\n",
    "    curr_chunk = []\n",
    "    for token, tag in named_entities:\n",
    "        if tag != 'O':\n",
    "            curr_chunk.append((token, tag))\n",
    "        else:\n",
    "            if curr_chunk:\n",
    "                cont_chunk.append(curr_chunk)\n",
    "                curr_chunk = []\n",
    "        if curr_chunk:\n",
    "            cont_chunk.append(curr_chunk)\n",
    "    n_e = [\" \".join([token for token, tag in ne]) for ne in cont_chunk]\n",
    "    nam_ent = set()\n",
    "    named_entities = [i for i in n_e if not (i in nam_ent or nam_ent.add(i))] \n",
    "    \n",
    "    return('Player: '+ str(player) + ' Team: '+ str(named_entities))\n",
    "\n",
    "#def position_of_the_player(doc): -- Not attempted as I have to move on to next assignment\n",
    "#  # your code goes here\n",
    "#  return position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-CNrMM5_WVO"
   },
   "source": [
    "Execute the cell below to verify the `date_of_birth` function for the third player. Expected output `5 February 1992`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jpeKE1u9_WVP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Player: [] DOB: ['5 February 1992']\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_of_birth(docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3VtWxBr_WVZ"
   },
   "source": [
    "## Task 6 (10 Marks)\n",
    "Identify one other relation (besides team and player) and write a function to extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TR0GZrUB_WVa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player:['Cristiano Ronaldo dos Santos Aveiro'] Lang: ('Portuguese', 40)\n"
     ]
    }
   ],
   "source": [
    "# bit of a cheat as its just the same code block I used above except swapping our country for language\n",
    "def language_of_the_player(doc):\n",
    "    player = name_of_the_player(doc)\n",
    "    tagged_words = []\n",
    "    if any(isinstance(i, list) for i in doc) == True:\n",
    "        for i in doc:\n",
    "            for j in i:\n",
    "                tokenized_text = word_tokenize(j)\n",
    "                classified_text = st.tag(tokenized_text)\n",
    "                if len(classified_text) > 0:\n",
    "                    tagged_words.append(classified_text)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "        root1 = []\n",
    "        for i in tagged_words:\n",
    "            for j in i:\n",
    "                # not great only identifies languages/nationality of countries with these endings\n",
    "                if j[0][-3:] in ('ish','ese','ine','ian'):\n",
    "                    word_root = j[0][:4]\n",
    "                    word = j[0][0]\n",
    "                    root1.append((word_root, word))\n",
    "                    \n",
    "        root2 = []\n",
    "        for i in tagged_words:\n",
    "            for j in i:\n",
    "                place = [x for x in j if j[1]=='LOCATION']\n",
    "                if len(place) > 0 and len(place[0]) > 4:\n",
    "                    word_root = place[0][:4]\n",
    "                    word = place[0]\n",
    "                    root2.append((word_root, word))\n",
    "    \n",
    "        lang = []\n",
    "        for i in root1:\n",
    "            for j in root2:\n",
    "                if i[0] == j[0]:\n",
    "                    lang.append(i[1])\n",
    "        \n",
    "        freq = Counter(lang)\n",
    "        language = freq.most_common(1)[0]\n",
    "        \n",
    "        return ('Player:' + str(player) + 'Lang: ' + str(language))\n",
    "    \n",
    "    else:\n",
    "        for i in doc:\n",
    "            j = str(i)\n",
    "            tokenized_text = word_tokenize(j)\n",
    "            classified_text = st.tag(tokenized_text)\n",
    "            if len(classified_text) > 0:\n",
    "                tagged_words.append(classified_text)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        root1 = []\n",
    "        for i in tagged_words:\n",
    "            for j in i:\n",
    "                if j[0][-3:] in ('ish','ese','ine','ian','ina'):\n",
    "                    word_root = j[0][:4]\n",
    "                    word = j[0]\n",
    "                    root1.append((word_root,word))\n",
    "        \n",
    "        root2 = []\n",
    "        for i in tagged_words:\n",
    "            for j in i:\n",
    "                place = [x for x in j if j[1]=='LOCATION']\n",
    "                if len(place) > 0 and len(place[0]) > 4:\n",
    "                    word_root = place[0][:4]\n",
    "                    word = place[0]\n",
    "                    root2.append((word_root, word))\n",
    "    \n",
    "        lang = []\n",
    "        for i in root1:\n",
    "            for j in root2:\n",
    "                if i[0] == j[0]:\n",
    "                    lang.append(i[1])\n",
    "        \n",
    "        freq = Counter(lang)\n",
    "        language = freq.most_common(1)[0]\n",
    "        \n",
    "        return ('Player:' + str(player) + ' Lang: ' + str(language))\n",
    "    \n",
    "print(language_of_the_player(docs[0]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 2.ipynb",
   "provenance": [
    {
     "file_id": "1EXXdimBbQY8nnqIs5hBXdG68r2hsk7HS",
     "timestamp": 1604940588321
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
